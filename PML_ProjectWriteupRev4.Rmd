---
title: "Practical Machine Learning: Multi-class Recognition"
author: DJR
output:
  html_document: 
    fig.keep: all
    fig.path: figure/
    keep_md: yes
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```

##Session Information  
**Created:** `r date()`  
**Rstudio Version:** 1.1.453 <http://www.rstudio.com> 

# Load libraries
```{r libraries, message= FALSE}
library(ggplot2);library(dplyr);library(lubridate);library(caret);library(RANN);library(PerformanceAnalytics)
library(parallel);library(doParallel) #needed for faster random forest convergence
```
# Summary

*A training data set for six individuals is available. The instruments were placed on the belt, forearm, arm, and dumbell. Five classes are defined for characterizing qualitative activity recognition.
* Linear and Quadratic Discriminant Analysis (**LDA, QDA**) models for classifying the form in which a dumbell lifting excercise were trained using data from instruments. Out of sample accuracy is estimated at 75% approximatley for these models, which is deemed insufficient for getting good results on the quiz predictions.
* Default cross-validation is used to asses the generality of these model fits.
* The original paper associated to this data:<http://web.archive.org/web/20170519033209/http://groupware.les.inf.puc-rio.br:80/public/papers/2013.Velloso.QAR-WLE.pdf>, used a bagged random forest approach that achieved 78.2% overall accuracy. This is a good benchmark for the model used here.
* To obtain better accuarcy  a model using the **rf** method, with 5-fold crossvalidation and multicore parallel processing for reasonable convergence time.
* The expected out of sample error prediction for the better performig **rf** model is calculated. 
* Prediction on the testing data set by the **rf** model is used to answer teh quiz questions 

# Load Dataset
Initial inspection of the dataframe summary (not shown here for brevity) reveals a large number of divide by zero entries. These are reassigned as NA entries before further cleanup of the data.
``` {r load, cache=TRUE }
na_strings<-c("#DIV/0!","NA")
training <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", na.strings=na_strings)
testing <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", na.strings=na_strings)

```

##Data Cleanup
A few columns are reassigned apprporate classes as factors or POSIXct. Near zero variance columns that will contribute little to explaining the outcome are removed.
``` {r dataCleanup, cache=TRUE, dependson=c("load")}

trainingClean<-mutate(training, user_name=as.factor(user_name),classe=as.factor(classe),new_window=as.factor(new_window),cvtd_timestamp=dmy_hm(cvtd_timestamp))
testingClean<-mutate(testing,user_name=as.factor(user_name),new_window=as.factor(new_window),cvtd_timestamp=dmy_hm(cvtd_timestamp))
trainingClean <- trainingClean[,-nearZeroVar(trainingClean)] #remove columns with near zero variance in the TRAINING set.
#testingClean <- testingClean[,-nearZeroVar(testingClean)] #remove columns with near zero variance in the testing set.

```

# Exploratory Analysis
##Factors' representation in training data 
``` {r factors, cache=TRUE, dependson=c("dataCleanup")}
table(trainingClean$user_name,trainingClean$classe)
cleanColumns<-dim(trainingClean)[2]
cleanRows<-dim(trainingClean)[1]
#testRows<-dim(testingClean)[1]
```
* Seems like the data is reasonably balanced among factors. Random sampling shuold be suitable for splitting the data for cross validation purposes.  
* It is not practical to look at correlation plots since there are `r format(cleanColumns,digits=0, nsmall=0)` columns in the cleaned-up data. 

## Outcome correlation with Participant and Timestamp

``` {r corrChart, cache=TRUE, dependson=c("dataCleanup")}
sub<-select(trainingClean, user_name,raw_timestamp_part_1,raw_timestamp_part_2,classe)
sub<-mutate(sub,user_name=as.integer(user_name), classe=as.integer(classe))
suppressWarnings(chart.Correlation(sub, histogram=TRUE, pch=19))


```

* No apparent correlation exists between the outcome, participant, and the time stamp. However, the model should only use instrument motion data to classify the outcome. Therefore, the timestamp columns and participant factor should be excluded form the cleaned-up training set.  
* Since the cleaned up columns (except for factors and timestamps) are numerical, model-based linear classification methods  seem appropriate. These methods assume multivariate **Gaussian** parameters with same covariances (**LDA**) or different covariances(**QDA**). 
* To fulfill the Gaussian assumption **standardizing (center and scale) + BoxCox** preprocessing is used.  
* Since there is missing data in many columns, **nearest neighbors inpute** is used.
* Since we have a reasonably large size of well-balanced training cases (`r format(cleanRows,digits=0, nsmall=0)` rows), a **single cross-validation set** can be randomly sampled from it to **estimate out of sample error**. 

``` {r preprocessingR, cache=TRUE, dependson=c("dataCleanup")}
set.seed(7986)

#Remove timestamp columns and participant factor
trainingPre<-select(trainingClean,-X,-user_name,-classe, -raw_timestamp_part_1,-raw_timestamp_part_2, -cvtd_timestamp)

testingPre<-select(testingClean,names(trainingPre)[1:length(trainingPre)-1])

#Create pre-processing object
#PreObj_StdBoxCox<-preProcess(trainingPre,method = c("center","scale","BoxCox", 'knnImpute'))
PreObj_StdBoxCox<-preProcess(trainingPre,method = c('knnImpute'))
#Preprocess
trainingPre <- predict(PreObj_StdBoxCox, newdata=trainingPre) #pre-processed parameters
trainingPre <-mutate(trainingPre, classe=training$classe) #restore outcome column
#Split data for cross-validation
#trainp<-0.75
#pcntCV<-(1-0.75)*100
#inTrain<-createDataPartition(trainingPre$classe,p=trainp,list=FALSE)
#training<-trainingPre[inTrain,]
#crossValidation<-trainingPre[-inTrain,]
#ARtraining<-dim(training)[1]/dim(training)[2]
ARtraining<-dim(trainingPre)[1]/dim(trainingPre)[2]


#* `r format(pcntCV,digits=0, nsmall=0)`% of the original training data is split for use as a cross-validation #set. No need for other resampling methods (e.g. k-fold) since the data set is not small. A judgement call is #made in favor of less variance (potentially larger prediction bias). 
#rm(trainingClean)
```
* We have over 10 times (`r format(ARtraining,digits=0, nsmall=0)`) more sample rows than predictor columns, as recommeded for the QDA covariance matrix to be invertible.

# Training
* Fitted a **LDA and QDA** models with preprocessing combining **center,scale, BoxCox, and PCA**. Training converged in less than a minute using a typical laptop PC.   
* Also tried training a model using the **treebag** method which did not converge within many minutes of runtime.
* Used 10 fold Cross-Validation resampling to assess the **out of sample error** and **kappa** metric.
* PCA is used to distill a regressor basis that captures most of the variance with optimum degrees of freedom.
* LDA expected performance is better but warnings of **collinearity** were present during the training. QDA converged with no warnings.
* 

## LDA Training
``` {r LDATrain, cache=TRUE, dependson=c("preprocessingR")}
#priorClasse<-c(1, 1, 1, 1, 1)/5 #Assume prior probabilities for excercise outcome (classe)
set.seed(342)

suppressWarnings(fitLDA<-train(classe~.,data=trainingPre,method="lda",preProcess=c("center","scale"),trControl = trainControl(method = "cv"))) #accuracy 0.7646 kappa 0.7023
print(fitLDA)
confusionMatrix(data=predict(fitLDA,trainingPre),reference=trainingPre$classe)
predLDAtraining<-predict(fitLDA,trainingPre)
```

## QDA TRAINING
```{r QDATrain,cache=TRUE, dependson=c("preprocessingR")}
fitQDA<-train(classe~.,data=trainingPre,method="qda",preProcess=c("center","scale","pca"),trControl = trainControl(method = "cv")) #accuracy 0.7498 kappa 0.6870
print(fitQDA)
confusionMatrix(data=predict(fitQDA,trainingPre),reference=trainingPre$classe)
predQDAtraining<-predict(fitQDA,trainingPre)
```

## Combined Model
The LDA and QDA predictions could be used in a combineed model for better acccuracy but the benefit will be marginal since the predictions are highly correlated (see plot below):
``` {r combinedMOdel, cache=TRUE, dependson=c("LDATrain","QDATrain","preprocessingR")}
qplot(predLDAtraining,predQDAtraining,colour=classe, data=trainingPre,geom="jitter")
```
Therefore, no combined LDA + QDA model was be trained.

## RF Training

* The guidelines provided by mentor L. Greski in the project forum indicate that:  
        + The algorithm needs least 99% accuracy to have a reasonable probability of obtaining a perfect score on the quiz. Therefore, the LDA and QDA performance will fall short.  
        + Parallel, multicore processing is needed for shorter training convergence time.
        + 5 fold cross-validation resampling is a good compromise for good model performance prediction without slowing down the training too much.
```{r RFTrain,cache=TRUE, dependson=c("preprocessingR")}

#Following guidelines by mentor L. Greski on project forum.

#Setup parallel processing cluster
x <- trainingPre[,-119]
y <- trainingPre[,119]
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

#Setup 5-fold cross validation and enable parallel processing.
set.seed(465)
fitControl <- trainControl(method = "cv", number = 5, allowParallel = TRUE) # 5 folds of cross vallidation are teh recommended tradeoff between model performance and training speed.

#Fit RF
fitRF<- train(x,y, method="rf",data=trainingPre,trControl = fitControl)

#De-register parallel processing cluster to restore single core processing
stopCluster(cluster)
registerDoSEQ()

#Model Perfomance Check
fitRF
fitRF$resample
confusionMatrix.train(fitRF)

```
* Out of sample accuracy estimated as 99.73%
* Out of sample error estimated as 0.27%

# Test Set Prediction
``` {r predictTest, cache=TRUE, dependson=c("RFTrain", "preprocessingR")}
#For Quiz Only

#TestPrediction<-predict(fitRF, newdata=testingPre)
#TestPrediction
```


